[dataset]
drop_last = True
seq_len= 120
batch_size = 128
num_workers = 4
feature_num = 9
num_targets = 6
; Hz
raw_sampling_rate = 50
multi_sampling_rates = [[50,50,50],[10,10,10],[5,5,5]]
task = classification
;args: {stages, device, pooling_type, data_shape, d_model, kernel_size, layers_per_encoder, attn_heads, d_ffn, dropout, enable_res_parameter})
;  -> None

[model]
multi_rate_groups = 3
;mean,cat,
pooling_type = mean
d_model = 32,48,60
kernel_size = 5,1,1
layers_per_encoder = 4
attn_heads = 4
d_ffn = 256
dropout = 0.2
enable_res_parameter = 1
; momentum rate
momentum = 0.99
;loss
lambda = 1,0.3,0.1

[training]
; which device for training, cpu/cuda
device= cuda
; learning rate
lr = 0.001
; weight decay used in optimizer
weight_decay = 0
; max num of training epochs
epochs = 100
; max num of pre_training epochs
num_epoch_pretrain = 100
; what type of optimizer to use, adam/adamw
optimizer_type = adamw

beta = 0.2,1
